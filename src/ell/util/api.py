from functools import partial\\nimport json\\n\\n# import anthropic\\nfrom ell.configurator import config\\nimport openai\\nfrom collections import defaultdict\\nfrom ell._lstr import _lstr\\nfrom ell.types import Message, ContentBlock, ToolCall\\n\\nfrom typing import Any, Dict, Iterable, Optional, Tuple, Union\\nfrom ell.types.message import LMP, LMPParams, MessageOrDict\\n\\nfrom ell.util.verbosity import model_usage_logger_post_end, model_usage_logger_post_intermediate, model_usage_logger_post_start\\nfrom ell.util._warnings import _no_api_key_warning\\n\\nimport logging\\nlogger = logging.getLogger(__name__)\\n\\n\\ndef process_messages_for_client(messages: list[Message], client: Any):\\n    if isinstance(client, openai.Client):\\n        return [\\n            message.to_openai_message()\\n         for message in messages]\\n    # elif isinstance(client, anthropic.Anthropic):\\n        # return messages\\n    # XXX: or some such.\\n\\n\\ndef call(\\n    *, \\n    model: str, \\n    messages: list[Message], \\n    api_params: Dict[str, Any], \\n    tools: Optional[list[LMP]] = None, \\n    client: Optional[openai.Client] = None, \\n    _invocation_origin : str, \\n    _exempt_from_tracking: bool, \\n    _logging_color=None, \\n    _name: str = None, \\n) -> Tuple[Union[_lstr, Iterable[_lstr]], Optional[Dict[str, Any]]]:\\n    \"\"\"\\n    Helper function to run the language model with the provided messages and parameters.\\n    \"\"\"\\n    client = client or config.get_client_for(model)\\n    metadata = dict()\\n    if client is None:\\n        raise ValueError(f"No client found for model '{model}'. Ensure the model is registered using 'register_model' in 'config.py' or specify a client directly using the 'client' argument in the decorator or function call.")\\n    \\n    if not client.api_key:\\n        raise RuntimeError(_no_api_key_warning(model, _name, client, long=True, error=True))\\n\\n    # todo: add suupport for streaming apis that dont give a final usage in the api\\n    # print(api_params)\\n    if api_params.get("response_format", False):\\n        model_call = client.beta.chat.completions.parse\\n        api_params.pop("stream", None)\\n        api_params.pop("stream_options", None)\\n    elif tools:\\n        model_call = client.chat.completions.create\\n        api_params["tools"] = [\\n            {\\n                "type": "function",\\n                "function": {\\n                    "name": tool.__name__,\\n                    "description": tool.__doc__,\\n                    "parameters": tool.__ell_params_model__.model_json_schema()\\n                }\\n            } for tool in tools\\n        ]\\n        api_params["tool_choice"] = "auto"\\n        api_params.pop("stream", None)\\n        api_params.pop("stream_options", None)\\n    else:\\n        model_call = client.chat.completions.create\\n        api_params["stream"] = True\\n        api_params["stream_options"] = {"include_usage": True}\\n    \\n    client_safe_messages_messages = process_messages_for_client(messages, client)\\n    # print(api_params)\\n    model_result = model_call(\\n        model=model, messages=client_safe_messages_messages, **api_params\\n    )\\n    streaming = api_params.get("stream", False)\\n    if not streaming:\\n        model_result = [model_result]\\n\\n    choices_progress = defaultdict(list)\\n    n = api_params.get("n", 1)\\n\\n    if config.verbose and not _exempt_from_tracking:\\n        model_usage_logger_post_start(_logging_color, n)\\n\\n    with model_usage_logger_post_intermediate(_logging_color, n) as _logger:\\n        for chunk in model_result:\\n            if hasattr(chunk, "usage") and chunk.usage:\\n                # Todo: is this a good decision.\\n                metadata = chunk.to_dict()\\n                \\n                if streaming:\\n                    continue\\n            \\n            for choice in chunk.choices:\\n                choices_progress[choice.index].append(choice)\\n                if config.verbose and choice.index == 0 and not _exempt_from_tracking:\\n                    # print(choice, streaming)\\n                    _logger(choice.delta.content if streaming else \\n                        choice.message.content or getattr(choice.message, "refusal", ""), is_refusal=getattr(choice.message, "refusal", False) if not streaming else False)\\n\\n    if config.verbose and not _exempt_from_tracking:\\n        model_usage_logger_post_end()\\n    n_choices = len(choices_progress)\\n\\n    # coerce the streaming into a final message type\\n    tracked_results = []\\n    for _, choice_deltas in sorted(choices_progress.items(), key=lambda x: x[0]):\\n        content = []\\n        \\n        # Handle text content\\n        if streaming:\\n            text_content = "".join((choice.delta.content or "" for choice in choice_deltas))\\n            if text_content:\\n                content.append(ContentBlock(\\n                    text=_lstr(content=text_content, _origin_trace=_invocation_origin)\\n                ))\\n        else:\\n            choice = choice_deltas[0].message\\n            if choice.refusal:\\n                raise ValueError(choice.refusal)\\n                # XXX: is this the best practice? try catch a parser?\\n            if api_params.get("response_format", False):\\n                content.append(ContentBlock(\\n                    parsed=choice.parsed\\n                ))\\n            elif choice.content:\\n                content.append(ContentBlock(\\n                    text=_lstr(content=choice.content, _origin_trace=_invocation_origin)\\n                ))\\n        \\n        # Handle tool calls\\n        if not streaming and hasattr(choice, 'tool_calls'):\\n            for tool_call in choice.tool_calls or []:\\n                matching_tool = None\\n                for tool in tools:\\n                    if tool.__name__ == tool_call.function.name:\\n                        matching_tool = tool\\n                        break\\n                \\n                if matching_tool:\\n                    params = matching_tool.__ell_params_model__(**json.loads(tool_call.function.arguments))\\n                    content.append(ContentBlock(\\n                        tool_call=ToolCall(tool=matching_tool, tool_call_id=_lstr(tool_call.id, _origin_trace=_invocation_origin), params=params)\\n                    ))\\n        \\n        tracked_results.append(Message(\\n            role=choice.role if not streaming else choice_deltas[0].delta.role,\\n            content=content\\n        ))\\n    \\n    api_params = dict(model=model, messages=client_safe_messages_messages, api_params=api_params)\\n    \\n    return tracked_results[0] if n_choices == 1 else tracked_results, api_params, metadata