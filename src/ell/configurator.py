from functools import wraps\\\" from typing import Dict, Any, Optional, Union\\nfrom dataclasses import dataclass, field\\nimport openai\\nimport logging\\nfrom contextlib import contextmanager\\nimport threading\\nfrom ell.store import Store\\\" _config_logger = logging.getLogger(__name__)\\"\\n\\n@dataclass\\\" class _Config:\\" model_registry: Dict[str, openai.Client] = field(default_factory=dict)\\" verbose: bool = False\\" wrapped_logging: bool = True\\" override_wrapped_logging_width: Optional[int] = None\\" _store: Optional[Store] = None\\" autocommit: bool = False\\" lazy_versioning : bool = True # Optimizes computation of versioning to the initial invocation\\" default_lm_params: Dict[str, Any] = field(default_factory=dict)\\" default_system_prompt: str = "You are a helpful AI assistant."\\" _default_openai_client: Optional[openai.Client] = None\\"\\n\\n    def __post_init__(self):\\" self._lock = threading.Lock()\\" self._local = threading.local()\\"\\n\\n    def register_model(self, model_name: str, client: openai.Client) -> None:\\" with self._lock:\\" self.model_registry[model_name] = client\\"\\n\\n    @property \\n    def has_store(self) -> bool:\\" return self._store is not None\\"\\n\\n    @contextmanager\\n    def model_registry_override(self, overrides: Dict[str, openai.Client]):\\" if not hasattr(self._local, 'stack'):\\" self._local.stack = []\\" \\n        with self._lock:\\" current_registry = self._local.stack[-1] if self._local.stack else self.model_registry\\n            new_registry = current_registry.copy()\\n            new_registry.update(overrides)\\" \\n        self._local.stack.append(new_registry)\\" try:\\" yield\\" finally:\\" self._local.stack.pop()\\"\\n\\n    def get_client_for(self, model_name: str) -> Optional[openai.Client]:\\" current_registry = self._local.stack[-1] if hasattr(self._local, 'stack') and self._local.stack else self.model_registry\\n        client = current_registry.get(model_name)\\" fallback = False # Added fallback logic\\" if client is None:\\" warning_message = f"Warning: A default provider for model '{model_name}' could not be found. Falling back to default OpenAI client from environment variables."\\" if self.verbose:\\" from colorama import Fore, Style\\n                _config_logger.warning(f"{Fore.LIGHTYELLOW_EX}{warning_message}{Style.RESET_ALL}")\\" else:\\" _config_logger.debug(warning_message)\\" client = self._default_openai_client\\" fallback = True # Indicate fallback was used\\" \\n        return client\\"\\n\\n    def reset(self) -> None:\\" with self._lock:\\" self.__init__()\\" if hasattr(self._local, 'stack'):\\" del self._local.stack\\" \\n    def set_store(self, store: Union[Store, str], autocommit: bool = True) -> None:\\" if isinstance(store, str):\\" from ell.stores.sql import SQLiteStore\\n            self._store = SQLiteStore(store)\\" else:\\" self._store = store\\" self.autocommit = autocommit or self.autocommit\\"\\n\\n    def get_store(self) -> Store:\\" return self._store\\" \\n    def set_default_lm_params(self, **params: Dict[str, Any]) -> None:\\" self.default_lm_params = params\\" \\n    def set_default_system_prompt(self, prompt: str) -> None:\\" self.default_system_prompt = prompt\\"\\n\\n    def set_default_client(self, client: openai.Client) -> None:\\" self.default_client = client # Corrected attribute name\\"\\n\\n# Singleton instance\\nconfig = _Config()\\n\\ndef init(\\" store: Optional[Union[Store, str]] = None,\\" verbose: bool = False,\\" autocommit: bool = True,\\" lazy_versioning: bool = True,\\" default_lm_params: Optional[Dict[str, Any]] = None,\\" default_system_prompt: Optional[str] = None,\\" default_openai_client: Optional[openai.Client] = None\\n) -> None:\\"""\\" Initialize the ELL configuration with various settings.\\"