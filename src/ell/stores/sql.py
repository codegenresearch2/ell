from datetime import datetime, timedelta\\nfrom typing import List, Optional, Dict, Any\\nfrom sqlmodel import SQLModel, create_engine, Session\\nimport ell.store\\nimport cattrs\\nimport numpy as np\\nfrom sqlalchemy.sql import text\\nfrom ell.types import InvocationTrace, SerializedLMP, Invocation, InvocationContents\\nfrom ell._lstr import _lstr\\nfrom sqlalchemy import or_, func, and_, extract\\nfrom sqlalchemy.types import TypeDecorator, VARCHAR\\nfrom ell.types.lmp import SerializedLMPUses, utc_now\\nfrom ell.util.serialization import pydantic_ltype_aware_cattr\\nimport gzip\\nimport json\\n\\nclass SQLStore(ell.store.Store):\\n    def __init__(self, db_uri: str, has_blob_storage: bool = False):\\n        self.engine = create_engine(db_uri, json_serializer=lambda obj: json.dumps(pydantic_ltype_aware_cattr.unstructure(obj), sort_keys=True, default=repr))\\n        SQLModel.metadata.create_all(self.engine) \\n        self.open_files: Dict[str, Dict[str, Any]] = {} \\n        super().__init__(has_blob_storage) \\n\\n    def write_lmp(self, serialized_lmp: SerializedLMP, uses: Dict[str, Any]) -> Optional[Any]:\\n        with Session(self.engine) as session:\\n            lmp = session.exec(select(SerializedLMP).filter(SerializedLMP.lmp_id == serialized_lmp.lmp_id)).first()\\n            if lmp:\\n                return lmp\\n            else:\\n                session.add(serialized_lmp) \\n            for use_id in uses:\\n                used_lmp = session.exec(select(SerializedLMP).where(SerializedLMP.lmp_id == use_id)).first()\\n                if used_lmp:\\n                    serialized_lmp.uses.append(used_lmp) \\n            session.commit()\\n        return None\\n\\n    def write_invocation(self, invocation: Invocation, consumes: Set[str]) -> Optional[Any]:\\n        with Session(self.engine) as session:\\n            lmp = session.exec(select(SerializedLMP).filter(SerializedLMP.lmp_id == invocation.lmp_id)).first()\\n            assert lmp is not None, f"LMP with id {invocation.lmp_id} not found. Writing invocation erroneously"\\n\\n            if lmp.num_invocations is None:\\n                lmp.num_invocations = 1\\n            else:\\n                lmp.num_invocations += 1\\n\\n            session.add(invocation.contents) \\n            session.add(invocation) \\n\\n            for consumed_id in consumes:\\n                session.add(InvocationTrace(\\n                    invocation_consumer_id=invocation.id,\\n                    invocation_consuming_id=consumed_id\\n                ))\\n\\n            session.commit()\\n            return None\\n\\n    def get_cached_invocations(self, lmp_id :str, state_cache_key :str) -> List[Invocation]:\\n        with Session(self.engine) as session:\\n            return self.get_invocations(session, lmp_filters={"lmp_id": lmp_id}, filters={"state_cache_key": state_cache_key})\\n\\n    def get_versions_by_fqn(self, fqn :str) -> List[SerializedLMP]:\\n        with Session(self.engine) as session:\\n            return self.get_lmps(session, name=fqn) \\n\\n    def get_latest_lmps(self, session: Session, skip: int = 0, limit: int = 10) -> List[Dict[str, Any]]:\\n        subquery = (\\n            select(SerializedLMP.name, func.max(SerializedLMP.created_at).label("max_created_at"))\\n            .group_by(SerializedLMP.name) \\n            .subquery()\\n        )\\n\\n        filters = {\"name": subquery.c.name, \\n                   "created_at": subquery.c.max_created_at\\n                  }\\n\\n        return self.get_lmps(session, skip=skip, limit=limit, subquery=subquery, **filters) \\n\\n    def get_lmps(self, session: Session, skip: int = 0, limit: int = 10, subquery=None, **filters: Optional[Dict[str, Any]]) -> List[Dict[str, Any]]:\\n        query = select(SerializedLMP) \\n\\n        if subquery is not None:\\n            query = query.join(subquery, and_(\\n                SerializedLMP.name == subquery.c.name, \\n                SerializedLMP.created_at == subquery.c.max_created_at\\n            ))\\n\\n        if filters:\\n            for key, value in filters.items():\\n                query = query.where(getattr(SerializedLMP, key) == value) \\n\\n        query = query.order_by(SerializedLMP.created_at.desc()) \\n        query = query.offset(skip).limit(limit) \\n        results = session.exec(query).all() \\n\\n        return results\\n\\n    def get_invocations(self, session: Session, lmp_filters: Dict[str, Any], skip: int = 0, limit: int = 10, filters: Optional[Dict[str, Any]] = None, hierarchical: bool = False) -> List[Dict[str, Any]]:\\n        query = select(Invocation).join(SerializedLMP) \\n\\n        if lmp_filters:\\n            for key, value in lmp_filters.items():\\n                query = query.where(getattr(SerializedLMP, key) == value) \\n\\n        if filters:\\n            for key, value in filters.items():\\n                query = query.where(getattr(Invocation, key) == value) \\n\\n        query = query.order_by(Invocation.created_at.desc()).offset(skip).limit(limit) \\n\\n        invocations = session.exec(query).all() \\n        return invocations\\n\\n    def get_traces(self, session: Session):\\n        query = text(\"\\n        SELECT \\n            consumer.lmp_id, \\n            trace.*, \\n            consumed.lmp_id\\n        FROM \\n            invocation AS consumer\\n        JOIN \\n            invocationtrace AS trace ON consumer.id = trace.invocation_consumer_id\\n        JOIN \\n            invocation AS consumed ON trace.invocation_consuming_id = consumed.id\\n        ") \\n        results = session.exec(query).all() \\n\\n        traces = []\\n        for (consumer_lmp_id, consumer_invocation_id, consumed_invocation_id, consumed_lmp_id) in results:\\n            traces.append({\"consumer": consumer_lmp_id, \\n                           \"consumed": consumed_lmp_id\\n                          }) \\n\\n        return traces\\n\\n    def get_all_traces_leading_to(self, session: Session, invocation_id: str) -> List[Dict[str, Any]]:\\n        traces = []\\n        visited = set()\\n        queue = [(invocation_id, 0)]\\n\\n        while queue:\\n            current_invocation_id, depth = queue.pop(0) \\n            if depth > 4:\\n                continue\\n\\n            if current_invocation_id in visited:\\n                continue\\n\\n            visited.add(current_invocation_id) \\n\\n            results = session.exec(\\n                select(InvocationTrace, Invocation, SerializedLMP) \\n                .join(Invocation, InvocationTrace.invocation_consuming_id == Invocation.id) \\n                .join(SerializedLMP, Invocation.lmp_id == SerializedLMP.lmp_id) \\n                .where(InvocationTrace.invocation_consumer_id == current_invocation_id) \\n            ).all()\\n            for row in results:\\n                trace = {\"consumer_id": row.InvocationTrace.invocation_consumer_id, \\n                         \"consumed": {key: value for key, value in row.Invocation.__dict__.items() if key not in ['invocation_consumer_id', 'invocation_consuming_id']},\\n                         \"consumed_lmp": row.SerializedLMP.model_dump()\\n                        }\\n                traces.append(trace) \\n                queue.append((row.Invocation.id, depth + 1))\\n\\n        unique_traces = {} \\n        for trace in traces: \\n            consumed_id = trace['consumed']['id'] \\n            if consumed_id not in unique_traces: \\n                unique_traces[consumed_id] = trace \\n\\n        return list(unique_traces.values()) \\n\\n    def get_invocations_aggregate(self, session: Session, lmp_filters: Dict[str, Any] = None, filters: Dict[str, Any] = None, days: int = 30) -> Dict[str, Any]:\\n        start_date = datetime.utcnow() - timedelta(days=days) \\n\\n        base_subquery = (\\n            select(Invocation.created_at, Invocation.latency_ms, Invocation.prompt_tokens, Invocation.completion_tokens, Invocation.lmp_id) \\n            .join(SerializedLMP, Invocation.lmp_id == SerializedLMP.lmp_id) \\n            .filter(Invocation.created_at >= start_date) \\n        )\\n\\n        if lmp_filters: \\n            base_subquery = base_subquery.filter(and_(*[getattr(SerializedLMP, k) == v for k, v in lmp_filters.items()]))\\n        if filters: \\n            base_subquery = base_subquery.filter(and_(*[getattr(Invocation, k) == v for k, v in filters.items()]))\\n\\n        data = session.exec(base_subquery).all() \\n\\n        total_invocations = len(data) \\n        total_tokens = sum(row.prompt_tokens + row.completion_tokens for row in data) \\n        avg_latency = sum(row.latency_ms for row in data) / total_invocations if total_invocations > 0 else 0 \\n        unique_lmps = len(set(row.lmp_id for row in data)) \\n\\n        graph_data = [] \\n        for row in data: \\n            graph_data.append({\"date": row.created_at, \\n                               \"avg_latency": row.latency_ms, \\n                               \"tokens": row.prompt_tokens + row.completion_tokens, \\n                               \"count": 1\\n                              }) \\n\\n        return {\"total_invocations": total_invocations, \\n                \"total_tokens": total_tokens, \\n                \"avg_latency": avg_latency, \\n                \"unique_lmps": unique_lmps, \\n                \"graph_data": graph_data\\n               }\\n\\nclass SQLiteStore(SQLStore):\\n    def __init__(self, db_dir: str):\\n        assert not db_dir.endswith('.db'), "Create store with a directory not a db." \\n        os.makedirs(db_dir, exist_ok=True) \\n        self.db_dir = db_dir \\n        db_path = os.path.join(db_dir, 'ell.db') \\n        super().__init__(f'sqlite:///{db_path}', has_blob_storage=True) \\n\\n    def _get_blob_path(self, id: str, depth: int = 2) -> str:\\n        if not self.has_blob_storage: \\n            raise ValueError("This store does not support external blob storage") \\n\\n        assert "-" in id, "Blob id must have a single - in it to split on." \\n        _type, _id = id.split("-") \\n        increment = 2 \\n        dirs = [_type] + [_id[i:i+increment] for i in range(0, depth*increment, increment)] \\n        file_name = _id[depth*increment:] \\n        return os.path.join(self.db_dir, "blob", *dirs, file_name) \\n\\n    def write_external_blob(self, id: str, json_dump: str, depth: int = 2):\\n        file_path = self._get_blob_path(id, depth) \\n        os.makedirs(os.path.dirname(file_path), exist_ok=True) \\n        with gzip.open(file_path, "wt", encoding="utf-8") as f: \\n            f.write(json_dump) \\n\\n    def read_external_blob(self, id: str, depth: int = 2) -> str:\\n        file_path = self._get_blob_path(id, depth) \\n        with gzip.open(file_path, "rt", encoding="utf-8") as f: \\n            return f.read()\\n